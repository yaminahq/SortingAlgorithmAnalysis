My observations were quite inaccurate. In fact, where my observations showed regular insertion sort to be the fastest, it actually turned out to be the slowest. Furthermore, my observations showed binary search to be the slowest but it turned out to be amongst the fastest. Several things could have caused these results. Although I did conduct three tests and then take the average of the execution time values for the three trials (to reduce the probability of a random fluke throwing off my results), there still could have been a chance that in the generated Job arrays, the random order may have been more optimal for some algorithms opposed to others. This is true because the best and worst case scenario for some algorithms depend on different input array structures. In the future, generating random job Array repeatedly and testing them as well as conducting many more trials would be a better approach. Additionally, it could be true that the size initially being tested, was simply not large enough to reflect the true nature of each algorithm. The results were greatly contrasted when the arrays of size 2^14 and 2^16 were used. We can be more confident in the larger array test cases as the random nature of the arrays will have a much smaller impact on the performance of each algorithm. This experiment made it clear to me why often approximating the performance of an algorithm is a much better approach, especially when testing for large values is not possible or too time-consuming. For my own interest, I plotted the execution times for the larger datasets and observed the change in the plots, as enclosed. This more accurate picture of the nature of each algorithm showed regular insertion sort to be O(n^2), insertion sort with comparable interface to be O(n^2), binary search insertion sort, merge sort and heap sort to be O(logn).

To further extend the accuracy of the hypothesized performance for each sorting implementation we can build mathematical models. By considering the algorithm at the core of each implementation, at itâ€™s worst case, we get regular insertion sort to be O(n^2), insertion sort with comparable interface to be O(n^2) and binary search insertion sort to be O(n^2). The three are bound by the two main loops they use, forcing a worst case runtime of n^2. Merge sort is found to be O(nlogn) as the entire array is traversed once and further split into halves to perform the sort. Heap sort is found to be O(nlogn) as once again the array is traversed completely about once and divided into halves to reheapify.

With all this considered, we can now rank the algorithms, best to worse. The fastest algorithm includes both merge sort and heap sort, with heap sort performing slightly better in the trials conducted. Following are the three insertion sort algorithms with binary search insertion sort performing significantly faster than the other two, followed by the regular insertion sort and finally the insertion sort using the comparable interface. This reveals a little about the nature of the Comparable class implementation. Furthermore it is intuitive that the binary search insertion sort will be faster in some cases as instead of a O(n) search a O(logn) search is conducted. However, many more exchanges occur as the array is traversed and the elements are shifted  one to the right.
